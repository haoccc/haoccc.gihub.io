---
title: 归一化（Normalization）
date: 2022-09-08 10:07:23
tags: ['归一化']
categories: ['深度学习']
index_img: /img/归一化.jfif
banner_img: /img/default.jpg
category_bar: true
---

归一化（输入归一化、层归一化、批归一化）：使得预处理的数据被限定在一定的范围内（比如[0,1]或者[-1,1]），从而消除奇异样本数据导致的不良影响。奇异样本数据的存在会引起**训练时间增大**，同时也可能导致**无法收敛**，因此，当存在奇异样本数据时，在进行训练之前需要对预处理数据进行归一化；反之，不存在奇异样本数据时，则可以不进行归一化。

## 输入归一化

不同输入特征的取值范围差异比较大时，会影响到梯度下降法的效率。以2维特征为例，特征取值不同会导致大多数位置的梯度方向并不是最优的搜索方向，当使用梯度下降法寻求最优解时，会导致需要很多次迭代才能收敛，如图(a)；当特征归一化后，梯度方向会近似为最优的搜索方向，如图(b)。

![](/img/归一化.jfif)

###  常见归一化方法

#### 极差变化法（Min-Max标准化）

​	线性变化，使得转换后特征的取值分布在$[0,1]$区间内。假设数据中特征$f$的取值集合为$\{f_1,f_2...,f_n\}$，特征取值$f_i$经过归一化后$f_i'$的取值为

$$f_i'={f_i-f_{min}\over f_{max}-f_{min}}$$

​	如果希望取值落在$[-1,1]$，可将上式修改为$f_i'=2{f_i-f_{min}\over f_{max}-f_{min}}-1$。

​	映射到任意区间$[a,b]$，则为$f_i'=(b-a){f_i-f_{min}\over f_{max}-f_{min}}+a$。

#### 0均值标准化（Z-score标准化）

$$f_i'={f_i-\mu\over \sigma}$$

其中，$\mu={1\over n}\sum\limits_{i=1} ^n f_i$为特征$f$的平均值，$\sigma=\sqrt[2]{\frac 1n\sum\limits_{i=1} ^n \(f_i-\mu\)^2}$为特征$f$的标准差。

## 批归一化（Batch Normalization）

神经网络都是多层结构，上一层 的输出即为下一层的输入，所以即使输入数据做了归一化，由于经过了线性变换以及激活函数，下一层的输入在取值范围可能又会有比较大的差别。从机器学习角度来看，如果某个神经层的输入分布发生了改变，那么其参数需要重新学习，这种现象叫做内部协变量偏移（Internal Covariate Shift）。

> 在传统机器学习中， 一个常见的问题是协变量偏移（Covariate Shift）。协变量是一个统计学概念，是可能影响预测结果的统计变量。在机器学习中，协变量可以看作是输入。一般的机器学习算法都要求输入在训练集和测试集上的分布是相似的。如果不满足这个假设，在训练集上学习到的模型在测试集上的表现会比较差。

为了解决内部协变量偏移问题，就要使得每一个神经层的输入的分布在训练过程中保持一致，最简单有效的方法就是逐层归一化。
**批归一化：** 批归一化（Batch Normalization）是对神经层中**单个神经元的输入（出）**进行归一化。

​	我们单从第$l$层第$i$个神经元$W_i^l$来看：上一个神经元的输出$a_i^{l-1}$，经过激活函数作为下一个神经元的输入$z_i^l=\sigma(a_i^{l-1})$，$\sigma$为激活函数。要避协变量偏移，就得对$z_i^l$进行归一化，**即归一化进行在线性变化之后，激活函数之前**，一般都用标准正态分布。

​	理想的情况是知道所有训练数据的$z_i^l$但是我们现在用的最多的是小批量梯度下降法，没法再迭代过程中知道所有的$z_i^l$，所以只能用batch size个数据来估计近似。假设有batch size=N，则N个训练数据的输入$z_i^l$分别为

$\{$z_i^l(1),z_i^l(2)...z_i^l(N)\}$

最后归一化的输入为

$$\hat{z_i^l}={z_i^l-\mu\over\sigma}$$

其中$\mu$为该批数据的均值，$\sigma$为该批数据的标准差。

​	对净输入的$z_i^l$标准归一化会使得其取值集中到0附近，如果使用sigmoid型激活函数时，这个取值区间刚好是接近线性变换的区间，减弱了神经网络的非线性性质。因此，为了使得归一化不对网络的表示能力造成负面影响，可以通过一个附加的缩放和平移变换改变取值区间。

$$\hat{z_i^l}={z_i^l-\mu\over\sigma}\lambda + \beta$$

其中$\lambda$和$\beta$分别代表缩放和平移的参数向量，是模型自己学习。


​	最后一点还需要注意的是，在使用BN后，神经网络的线性计算（WX + b）中的偏置b将不起作用，因为在（WX + b）求均值后b作为常数均值还是b，在规范化的过程中原数据要减去均值，所以b在这两步计算中完全抵消了。但由于BN的算法中有一个偏置项β，它完全可以代替b的作用，所以有BN的计算中可不用b。



#### 优点

- **BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度**：BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。

- **BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定**：在神经网络中，我们经常会谨慎地采用一些权重初始化方法（例如Xavier）或者合适的学习率来保证网络稳定训练。

- **BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题**：在不使用BN层的时候，由于网络的深度与复杂性，很容易使得底层网络变化累积到上层网络中，导致模型的训练很容易进入到激活函数的梯度饱和区；通过normalize操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题；另外通过自适应学习 γ 与 β 又让数据保留更多的原始信息。

  > [梯度饱和](https://zhuanlan.zhihu.com/p/111579675)：梯度饱和常常是和激活函数相关的，比如sigmod和tanh就属于典型容易进入梯度饱和区的函数，即自变量进入某个区间后，梯度变化会非常小，表现在图上就是函数曲线进入某些区域后，越来越趋近一条直线，梯度变化很小，梯度饱和会导致训练过程中梯度变化缓慢，从而造成模型训练缓慢

- **BN具有一定的正则化效果**：在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。


> [推荐阅读1](https://www.jianshu.com/p/491c7bc0e87c)
>
> [推荐阅读2](https://zhuanlan.zhihu.com/p/55852062)



## 层归一化（Layer Normalization）

批量归一化是对一个中间层的单个神经元进行归一化操作，因此要求小批量样本的数量不能太小，否则难以计算单个神经元的统计信息。层归一化（Layer Normalization）是和批量归一化非常类似的方法。和批量归一化不同的是，层归一化是对**某一层的所有神经元**进行归一化。

假设第$l$层有$M$个神经元，那么该层的输入

$$z^l=\{ z_1^l,z_2^l...z_M^l \}$$

求输入参数的均值$\mu$和方差$\sigma ^2$，和批归一化一样操作。同时层归一化也会附加一个缩放和平移变换的取值区间，操作方法与批归一化一样。

### 优点

在标准循环神经网络中，循环神经网络的净输入一般会随着时间慢慢变大或变小，从而导致梯度爆炸或消失。而层归一化的循环神经网络可以有效地缓解这种状况。

层归一化是基于批归一化进行优化得到的。相比较而言，批归一化是对一个神经元输入的数据以mini-batch为单位来进行汇总，计算平均值和方法，再用这个数据对每个训练样例的输入进行规整。层归一化在面对RNN等问题的时候效果更加优越，也不会受到mini-batch选值的影响。[*来源*](https://www.jiqizhixin.com/graph/technologies/edceaca3-7a3d-42b3-9189-d80b8b99a6ad)

相比较于批归一化，层归一化在**速度上不占有优势**。特别是在对CNN进行处理时，有实验证明，层规范化在面对ConvNet时，层归一化在效果和速度上都比批归一化效果差。

## 批归一化和层归一化区别

**批量归一化是不同训练数据之间对单个神经元的归一化，层归一化是单个训练数据对某一层所有神经元之间的归一化。** 所以，应当注意这两个“之间”，搞清楚到底是谁与谁进行求平均、方差，乃至归一化。

![批归一化和层归一化区别](/img/1662640261376.png)





> 参考文章:
>
> https://blog.csdn.net/a1367666195/article/details/105360306
>
> https://zhuanlan.zhihu.com/p/55852062
>
> https://www.jianshu.com/p/491c7bc0e87c


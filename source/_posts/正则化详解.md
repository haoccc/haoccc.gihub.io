---
title: 正则化（regularization）详解
date: 2022-09-07 21:31:58
tags: ['正则化']
categories: ['深度学习']
index_img: /img/正则化.png
category_bar: true
---


正则化防止过拟合（高方差），使拟合曲线更加平滑。

## 解决过拟合

- 减少变量的个数：舍弃一些变量，保留更为重要的变量。但是，如果每个特征变量都对预测产生影响。当舍弃一部分变量时，也就舍弃了一些信息。所以，希望保留所有的变量。
- 正则化：保留所有的变量，将一些不重要的特征的权值置为0或权值变小使得特征的参数矩阵变得稀疏（L<sub>1</sub>正则化），使每一个变量都对预测产生一点影响。
- dropout：神经网络有eval()和train()两种模式。计算预测值时记得切换到eval()模式，这种模式会关闭dropout；而在train()模式下，使用模型进行预测时仍然有部分神经元被dropout。
- 增大数据集

## 正则化

​	L<sub>1</sub>正则化：Lasso回归，所有参数的绝对值和$\|W\|=\sum_1^n\lvert w_i\lvert$。优点：可以使参数矩阵稀疏，挑选出重要的参数，使不重要参数为0。

​	L<sub>2</sub>正则化：岭回归，所有参数的平方和$\|W\|=\sum_1^n\lvert w_i^2\lvert$。优点：不仅可以防止过拟合，而且还使优化求解变得稳定，提高训练速度。

​	参数$w$的误差函数$J(w)$额外引入“正则”误差（惩罚项）$\lambda C$，其中$\lambda$是一个超参数，用来衡量惩罚项的重要程度，$C$为$\\sum_1^n\lvert w_i\lvert$或者$\sum_1^n\lvert w_i\lvert^2$。

$$J(w)=(\hat{y}-y)^2+\lambda C$$

## 正则化防止过拟合



​	添加正则项之后，更新$w$参数会使得$w$更小，具体的[公式推导。](https://blog.csdn.net/qq_37344125/article/details/104326946?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-104326946-blog-89482831.topnsimilarv1&spm=1001.2101.3001.4242.1&utm_relevant_index=3) $w$很小会使模型（function）是一个比较平滑的函数。

- 平滑性：一个平滑的函数，输入变化较大时输出的变化很小。

而我们的模型（function）越平滑，那么稳定性就越好，那么当输入变化过大，预测的结果那么仍然保持保持高效。

## $L_1$稀疏性

​	假设只有两个参数$w_1$和$w_2$需要学习，每个蓝色圆（等误差线）代表误差$(\hat{y}-y)^2$相同，黄线为正则化产生的额外误差（惩罚项），蓝黄线的交点处误差最小，即$w_1$和$w_2$正则化后的解。在$L_1$正则化中，黄色线突出的角比边更可能与蓝色圆相交，这个现象在高维的情况下更加明显，因为高维的角更加“突出”，由于角处于坐标轴上（导致部分参数取值为0），于是产生了稀疏解（很多参数为0）。

![左$L_2右$$L_1$正则化](/img/正则化详解/正则化.png)

## $L_2$稳定性

用批数据训练，每次训练数据都会有稍稍不同的误差曲线，以两个参数为例：$L_2$因为是圆形，最优解的变化不会很大，$L_1$的最优解可能会从一个角到另外一个角。

[莫烦Python：$L_2$稳定性：3：05](https://www.bilibili.com/video/BV1Tx411j7tJ?spm_id_from=333.337.search-card.all.click&vd_source=c4d2ace23e042712a3c8ffa7e56c035b)

## 常见问题

- 如何防止模型过拟合？
- 为什么正则化能够防止过拟合？数学角度
- 为什么$L_1$正则化具有稀疏性？



